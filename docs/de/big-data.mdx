# Big Data

## Introduction

Big data is a term used to describe datasets that are either:
- too large in size
- too complex in structure
- come to our system at such a high rate 
 
that exceeds the capacity of a traditional application to process it fast enough to provide any value.


## Characteristics

### Volume

Refers to the sheer amount of data that is being generated and processed daily. Ranges from terabytes to petabytes of data, or more.

Big data systems must be designed to store, process, and analyze massive datasets efficiently.

Examples of High-Volume Data:
- **Internet Search Engines**: Companies like Google process vast amounts of data from across the web to deliver instant search results.
- **Medical Systems**: Hospitals and clinics use software to collect and analyze patient data, helping with disease prevention and diagnosis.
- **Security Surveillance**: Real-time systems monitor video feeds from cameras across cities or secure facilities to detect and prevent crime.
- **Weather Forecasting**: Meteorological systems gather data from satellites and ground sensors to predict weather and issue alerts for storms or natural disasters.


### Variety

Variety refers to the diverse types of data that big data systems handle.

It includes:
- **Structured data**: neatly organized (e.g., rows and columns)
- **Unstructured data**: messy or free-form (e.g., text, images, videos)
- **Semi-structured data**: somewhere in between (e.g., JSON, XML)

Big data comes from many sources, a **data fusion** technique combines them into a unified view to helps uncover hidden patterns and valuable insights.

_For example, Social platforms collect diverse user behavior data (clicks, likes, shares, posts, time spent watching videos). These varied actions may seem unrelated, but when combined they help build predictive models for how users might respond to future content or ads._


### Velocity

Velocity refers to the speed at which data is generated, transmitted, and processed. It's about how fast that data flows into the system.

Big data systems often deal with **real-time or near-real-time data streams** to **ingest, analyze, and respond** to this data quickly enough to make timely decisions.

High-velocity data comes from:
- **Large-scale systems** like global e-commerce sites with millions of user interactions per second.
- **High-frequency devices** like IoT sensors in buses, trains, autonomous cars, constantly streaming location, speed, performance data.


## Methods

Methods to get insights from big data:

### Visualization
- Turns raw data into charts, graphs, and dashboards.
- Helps humans understand patterns that would be hard to spot in raw formats.

### Querying Capabilities
- Enables ad hoc analysis of large datasets.
- Useful when we don’t know what we’re looking for yet, helps uncover unexpected trends or patterns.

### Predictive Analysis
- Uses algorithms and machine learning to forecast user behavior and suggest actions (e.g., product recommendations).
- Can also detect anomalies in system logs to trigger alerts or rollbacks automatically.


## Data lake

A data lake is a centralized database that collects and stores massive amounts of data from any number of places.

It holds raw, unprocessed data of various types, including structured, semi-structured, and unstructured data.

It allows to search, analyze, visualize, and correlate your data on the fly.

After processing, data from a lake can be transferred to a data warehouse.


## Data Warehouse

A data warehouse is designed to store processed and refined data.

It serves as a repository for structured data that can be queried and analyzed for specific purposes.


## ETL

ETL (Extract, Transform, Load) is a process of extracting data from different source systems, transforming it to meet specific business requirements, and loading it into a target database or data warehouse for further analysis and reporting.

ETL processes are important for gaining valuable insights and making critical business decisions.
