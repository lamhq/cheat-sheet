# Caching Strategies

There are several caching strategies. Your caching strategy depends on the data and data access patterns. Choosing the right caching strategy is the key to improving performance.


## Cache-Aside

Application directly talks to both the cache and the database.

It checks the key in cache, and if it's not there, the application fetches it from the storage and then updates the cache.

![](https://codeahoy.com/img/cache-aside.png)

**Benefits**:
- Cache only what's needed
- Data model in cache can be different than the data model in database.

**Challenges**:
- **Complexity in Application Code**. The logic for checking the cache, fetching from DB, and populating the cache resides in the application
- **Data Staleness & Cache Invalidation**. Cached data can become stale if the source data in the database changes. Requires a strategy to invalidate or update the cache
- **Cache Miss Penalty**. The first request for uncached data (a miss) will incur both the cache check AND the database fetch
- **"Thundering Herd" Problem**. If a popular cached item expires, many concurrent requests might miss the cache simultaneously and all hit the database at once


## Read-Through

Client always interacts with the cache API. In case of a cache miss, the API loads data from the database, populates the cache, and returns it to the application.

It's a common pattern in ORM frameworks.

![](https://codeahoy.com/img/read-through.png)

**Benefits**:
- Cache only what's needed
- Transparent to the application

**Challenges**:
- Data model must be the same with database.
- Cache misses are expensive
- Data staleness


## Write-Through

Client interact with a cache API that also update the cache when data is written.

![](https://codeahoy.com/img/write-through.png)

Typically used in server-side caching, where updates happen within the same transaction.

**Benefits**:
- Cache always remains consistent with the database

**Challenges**:
- Writes are expensive: every write operation update both the cache and the database
- Redudant data: we write data to cache that never be read

**Where to use?**
- When data consistency is critical.
- When using server-side caching (e.g., Redis, Memcached).
- When read performance is more important than write speed.

> [!NOTE]
> DynamoDB Accelerator (DAX) is a good example of read-through / write-through cache.


## Write-Behind

A write-behind cache (aka write-back) updates the cache first, then asynchronously updates the origin later.

Data updates are batched and flushed to the database later.

![](https://codeahoy.com/img/write-back.png)

When combined with **Read-through**, it works good for mixed workloads, where the most recently updated and accessed data is always available in cache.

Most relational databases storage engines (i.e. InnoDB) have Write-Behind cache enabled by default in their internals. Queries are first written to memory and eventually flushed to the disk.

**Example**:
1. A User Preferences microservice updates a user's theme settings.
2. The cache is updated immediately, allowing fast retrieval.
3. The database is updated later, asynchronously.

**Use cases**:
- Good for write-heavy workloads.
- When batch updates to the origin are acceptable.
- When data loss is manageable (e.g., temporary session data).

**Benefits**:
- Improves write performance: writes are fast since they go to the cache first.
- Reduces load on the origin: batch updates to the database instead of frequent individual writes.
- Efficient for high-write workloads: useful when frequent updates occur but immediate persistence isnâ€™t required.

**Challenges**:
- Risk of data loss: if the cache fails before writing to the origin, data may be lost.
- Unclear source of truth: since the cache holds the latest data, the origin may contains stale data, which cause confusion.

> In Microservices, write-behind caching is rarely used, because consistency is prioritized, making delayed writes risky. Also, other caching strategies (e.g., write-through) are simpler and more reliable.