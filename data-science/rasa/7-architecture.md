# Overview

The two primary components are Natural Language Understanding (NLU) and dialogue management.

NLU is the part that handles intent classification, entity extraction, and response retrieval. It's shown below as the NLU Pipeline because it processes user utterances using an NLU model that is generated by the trained pipeline.

The dialogue management component decides the next action in a conversation based on the context. This is displayed as the Dialogue Policies in the diagram.

![](https://rasa.com/docs/rasa/img/architecture.png)


## Tracker Stores

Your assistant's conversations are stored within a tracker store. Rasa Open Source provides implementations for different store types out of the box, or you can create your own custom one.

- `InMemoryTrackerStore` (default): It stores the conversation history in memory.
- `SQLTrackerStore`: store your assistant's conversation history in an SQL database.
- `RedisTrackerStore`: Redis is a fast in-memory key-value store which can optionally also persist data.
- `MongoTrackerStore`: MongoDB is a free and open-source cross-platform document-oriented NoSQL database.


## Event Brokers

An event broker allows you to connect your running assistant to other services that process the data coming in from conversations. 

The event broker publishes messages to a message streaming service, also known as a message broker, to forward Rasa Events from the Rasa server to other services.

### Format

All events are streamed to the broker as serialized dictionaries every time the tracker updates its state. An example event emitted from the default tracker looks like this:

```json
{
  "sender_id": "default",
  "timestamp": 1528402837.617099,
  "event": "bot",
  "text": "what your bot said",
  "data": "some data about e.g. attachments"
  "metadata" {
    "a key": "a value",
  }
}
```

## Model Storage

Models can be stored in different places after you trained your assistant. This page explains how to configure Rasa to load your models.

### Load Model from Disk

By default models will be loaded from your local disk. You can specify the path to your model with the `--model` parameter:

```sh
rasa run --model models/
```

### Load Model from Server

You can configure the Rasa Open Source server to regularly fetch a model from a server and deploy it.

Rasa Open Source uses the `If-None-Match` and `ETag` headers for caching. Setting the headers will avoid re-downloading the same model over and over, saving bandwidth and compute resources.


### Load Model from Cloud

#### Amazon S3 Storage

Amazon S3 is supported using the boto3 package which you need to install as an additional dependency using pip3

```sh
pip3 install boto3
```

For Rasa to be able to authenticate and download the model, you need to set the following environment variables before running any command requiring the storage:

- `AWS_SECRET_ACCESS_KEY`: environment variable containing your AWS S3 secret access key
- `AWS_ACCESS_KEY_ID`: environment variable containing your AWS S3 access key ID
- `AWS_DEFAULT_REGION`: environment variable specifying the region of your AWS S3 bucket
- `BUCKET_NAME`: environment variable specifying the S3 bucket
- `AWS_ENDPOINT_URL`: The complete URL to use for the AWS S3 requests. You need to specify a complete URL (including the "http/https" scheme).

Once all environment variables are set, you can start the Rasa server:

```sh
rasa run --model 20190506-100418.tar.gz --remote-storage aws
```


## Using NLU Only

### Training

```sh
rasa train nlu
```

### Testing

```sh
rasa shell nlu
```

### Running an NLU server

```sh
rasa run --enable-api -m models/nlu-20190515-144445.tar.gz
```

You can then request predictions from your model using the `/model/parse` endpoint. To do this, run:

```sh
curl localhost:5005/model/parse -d '{"text":"hello"}'
```